what's up guys this is Ronnie welcome back to our Channel Total technology Zone this is tutorial 86 and in this tutorial I will show you how to use one of the very important L chain utility to extract all the URLs or sublinks from a given website okay so maybe uh some of you might ask this question that uh why we are actually covering this web scraping tutorial inside a AI tutorial right so yeah obviously this is kind of a web scraping but why I'm am doing this web scraping here just to uh just to prepare you guys for some sort of like complex project right suppose you have a website and uh you got a client and that client actually wants you to extract all the information from that website and after that uh the client wants to actually to put that data back to a rag for uh or a store not a rag to a store to create a rag based application so now to pre-process or to actually extract the data for your rag you obviously have to actually rely on that website right so if you don't know how to extract the data from the website that will be difficult right so that is why within the Lang chain we have couple of utilities and it's it's good actually that you should know how to use those utilities what are those utilities so that that will make you a complete developer so to give you guys kind of a complete uh like developer kind of experience I'm actually going to do this exciting tutorial okay so what we want to do here so basically we'll be going to use a document loader uh to actually um uh to actually extract all the URLs from a website so first we'll actually take a example website and from there we'll actually extract all the URL right and the code is actually available here nothing fancy will be going to actually do it okay so what I'll do I'll be going to actually open an incognito window and I'll go to Google and I'll be just going to write llama index documents so llama index actually has this link right so what we'll do we'll be going to actually extract all the URL from this link from this website okay so for that let me start writing the code so first thing first from Lang chain okay do community Dot document loaders okay dot recursive URL loader import recur URL loader right then from bs4 like we will be going to reord beautiful suit import beautiful soup okay not beautiful soup it is beautiful uh soup import beautiful soup s soup okay right H think uh it's kind of a okay done and then import iio right now what is actually going to happen we'll be going to actually create a variable called URL okay so this could be any URL right but I'm just going to give you the sample URL of llama index okay and after that just paste it here now I'll be going to extract all the links from this Ur right so then loader equals to recursive URL loader and inside that I'll be just going to pass couple of things like so URL equals to URL equals to URL equals to URL right so it requires some of the additional parameter as well arguments as well so basically we need to pass something called uh something called max depth so max depth is actually th000 okay means how many URLs you want to extract okay then uh let's see uh we'll be going to actually write a Lambda function so what it do and then like this and then we'll write extractor equals to Lambda X colon so okay like this x like this and then html. parer okay so I want to use HTML par and then txt right right so ultimately that is the thing okay these are the thing okay so let me actually run this thing and see whether I'm getting any error or not so no error is coming so everything is fine so what we'll do now so docs equals to loader do load okay so I got this information from the Lang chain documentation so that is why you have to actually read the documentation but some of you are actually not that much like U uh having that much of patience so you guys requested me that is there anything like uh to extract all the data from the website right so for that first you have to understand how many links are there so that will if you know the links it will be easier actually right so so everything is done now I'm actually loading the docs here so after that let's see how it is actually going to do okay so let's see whether I'm able to get everything here or not okay let's see okay so if you see here immediately it is actually extracting all the links here right okay so let's stop here so this hair is actually coming from the documentation okay right so done so terminal now couple of things we'll be going to do so basically um I have to actually open a file so with io. open right so I'll be going to write url. txt right then W then encoding equals to UTF 58 right okay then next thing is actually for Doc in docs right then F1 dot right so basically this docs actually having an element called metadata right so doc Dot metadata and within the metadata uh okay so I don't know like basically so what I'll do let me just do this thing here with a comment and let's print this docs actually okay so we understand what is there so let's see okay so I don't know how many links will be there but this will take some time but because there are lot of links are there so basic basically um uh what it do okay so let me remove this length to let's say five and see what is actually going to happen kill terminal okay new terminal and do 2 3 4 okay so it's still going so I don't know why it is not stopped here so what is this max depth okay okay so what is what is this max depth I don't know so okay so just keep on giving you this thing so there is no point whether you're giving it five or 10 right so that's something I want to actually give you guys kind of uh like understanding that Max dep actually okay so max depth is basically like from a single directory how many levels it will go maybe that is the same so it is better to actually make it let's say th000 is fine right so now let's stop this thing so what I'll do uh I need to actually print this thing somewhere right so doc. metadata and inside that there is something called source so basically it is only going to give you the uh Source right so source okay and after that I'll be just going to like this this sln right SL name will be like this and then what will happen we have to just close this thing right okay that's it now let it run so it will actually extract the entire thing and after that the all urls will be loaded here so depending on the links within this website uh this will take some time right so if you have th000 URL so maybe it is going to take maybe 20 minutes or something so if you see here it is keep on writing this thing right so let it run and after that we'll be able to see it what is happening okay so let's wait for some time okay okay so it's still going going like this okay than okay okay so still it is running so a lot of vs are there right so I won't actually stop the video over here because if in between if you get some error we need to fix it so that is why I want you guys to actually uh watch this full video okay and if you want to skip you can skip but it's better to actually uh read everything yeah you want to actually skip this docs part because um it is going to be printed later on but don't think this things are getting printed because of this line this is getting printed because of this thing uh it has this thing extraction part okay okay so let's see still working means it is still not hit this line when it is actually going to hit this line here you will see a file is created okay so let's wait okay okay so let it go as as it is and uh I will wait for another 2 minute and after that I will just top this thing here I mean the recording and after that I will actually going to uh see you guys at the end means I want to actually show you guys the output of this line because how I actually came to this conclusion you will understand because I know the documentation so that is why I directly wrote this thing but how I know because I printed this thing okay so let's see you in the next part of the video I'm just going to stop it here and I'll see you in the next part okay okay so let it run okay all right so the execution of that code has been completed over here okay so if you see uh at the end we have something called metadata so that is why I wrote doc. metadata inside that there is a key call source and inside that you have this URL so that is why I wrote like this so how many URLs I got so basically if you go here it actually able to F this many URLs so I don't know how manys are there but okay so almost like uh 1, 1,6 URLs are there right so it's amazing right so I would encourage all of you to try out this utility and see whether you are able to actually extract all the URLs from a website or not now if you're able to extract all the URLs now you have to actually only run the like chromium Asing chromium loader from the Lang change to get the information from all these websites right so these things will be very useful so I'll be going to share the codee all of you with all of you and uh the GitHub link is also provided in the PowerPoint slide so just try and practice okay so this is just entry point of creating a rack from website so if you know how to extract the data from the website so it will be very useful so that is why I'm just uh showing you guys how to actually uh uh like um create your data or prepare your data for your rag right so hopefully you know how to develop the rag already but now you know also how to actually get the data from the website with the help of Lang chain only right okay so with that note I'll be just going to conclude the video over here we'll see you in the next video but before I end this video here I would like to request couple of things uh very important thing uh try to subscribe to our Channel if you new you must subscribe to our Channel but if you are an existing subscri existing audience but you are not subscribing to our Channel please subscribe because your one subscription will help me to actually reach large number of audience and your one subscription will actually help me to uh get aligned with the YouTube algorithm so YouTube will also start promoting or pushing my videos to the large number of audience so please help me to grow I will always help you guys uh with um with new and exciting tutorial so that you will gain some knowledge for your upcoming projects or if you already started working on the client work that is also going to be helpful okay and next thing good or bad doesn't matter try to put some honest and valuable feedback so that I will understand how my videos are doing what are the things needs to be changed how I can actually do something better in the upcoming videos and if you still find something useful for my video from my video please let us know that as well so that will also help us to um motivate ourself L right okay so with that note I'll be just going to end the video over here we will see you in the next video till then take care goodbye have a nice day and happy lovely happy weekend